{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation - 06.02.2020\n",
    "Applied Deep Learning in Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import *\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:MAIN.PREPR:loading texts\n",
      "DEBUG:MAIN.PREPR:tokenizing texts\n",
      "DEBUG:MAIN.PREPR:normalizing text\n",
      "DEBUG:MAIN.PREPR:normalizing text done. returning 3246352 words\n",
      "DEBUG:MAIN.PREPR:building dataset\n",
      "DEBUG:MAIN.PREPR:unique words: 22934\n",
      "DEBUG:MAIN.PREPR:most common words: [['UNK', 21540], ('the', 171876), ('and', 146160), ('of', 88895), ('to', 77188), ('you', 69992), ('a', 60877), ('is', 52879), ('they', 49698), ('in', 45851)]\n",
      "DEBUG:MAIN.PREPR:building dataset done\n"
     ]
    }
   ],
   "source": [
    "# recreate dictionaries model was trained with\n",
    "input_dir = \"input/islam\"\n",
    "vocab_size = 10000\n",
    "words = get_word_tokens(input_dir)\n",
    "words = normalization(words)\n",
    "_, _, word2id, id2word = build_dataset(words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:EVALUATOR:loading keras model from path output/islam/models/final/model_05_02_2020_184210_Epoch12Ws3IslamBatch256.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 300)       3000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 300, 1)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 300, 1)       0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,000,000\n",
      "Trainable params: 3,000,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "DEBUG:EVALUATOR:loaded keras model\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "model_path = \"output/islam/models/final/model_05_02_2020_184210_Epoch12Ws3IslamBatch256.h5\"\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---NETWORK CONTEXT PREDICTIONS---\n",
      "\n",
      "good\n",
      "-he: \t 0.9856495\n",
      "-she: \t 0.9681358\n",
      "\n",
      "power\n",
      "-he: \t 0.9949201\n",
      "-she: \t 5.0696905e-05\n",
      "\n",
      "mighty\n",
      "-he: \t 0.99918073\n",
      "-she: \t 0.99999833\n",
      "\n",
      "bad\n",
      "-he: \t 0.99472153\n",
      "-she: \t 1.0\n",
      "\n",
      "evil\n",
      "-he: \t 0.9731208\n",
      "-she: \t 0.70281523\n",
      "\n",
      "lord\n",
      "-he: \t 0.9818392\n",
      "-she: \t 0.97953314\n",
      "\n",
      "god\n",
      "-he: \t 0.9766117\n",
      "-she: \t 0.9943382\n"
     ]
    }
   ],
   "source": [
    "# evaluate network prediction \n",
    "male_word = \"he\"\n",
    "male_word_id = word2id[male_word]\n",
    "female_word = \"she\"\n",
    "female_word_id = word2id[female_word]\n",
    "\n",
    "test_words = [\"good\", \"power\", \"mighty\", \"bad\", \"evil\", \"lord\", \"god\"]\n",
    "\n",
    "print(\"---NETWORK CONTEXT PREDICTIONS---\")\n",
    "for word in test_words:\n",
    "    word_id = word2id[word]\n",
    "    print(\"\\n\" + word)\n",
    "    print(f\"-{male_word}: \\t\", is_context_word(model, word_id, male_word_id))\n",
    "    print(f\"-{female_word}: \\t\", is_context_word(model, word_id, female_word_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.9999999, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# check cosine similarity, should produce 1\n",
    "print(cos_similarity(male_word_id, male_word_id, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---COSINE SIMILARITIES---\n",
      "\n",
      "good\n",
      "-he: \t 0.15852162\n",
      "-she: \t 0.03075388\n",
      "\n",
      "power\n",
      "-he: \t 0.12747066\n",
      "-she: \t -0.057414874\n",
      "\n",
      "mighty\n",
      "-he: \t 0.14584734\n",
      "-she: \t 0.06545416\n",
      "\n",
      "bad\n",
      "-he: \t 0.1051482\n",
      "-she: \t 0.08670533\n",
      "\n",
      "evil\n",
      "-he: \t 0.12937585\n",
      "-she: \t 0.0074573746\n",
      "\n",
      "lord\n",
      "-he: \t 0.3070649\n",
      "-she: \t 0.07154995\n",
      "\n",
      "god\n",
      "-he: \t 0.31637102\n",
      "-she: \t 0.10531166\n"
     ]
    }
   ],
   "source": [
    "# evaluate cosine similarity\n",
    "print(\"---COSINE SIMILARITIES---\")\n",
    "for word in test_words:\n",
    "    word_id = word2id[word]\n",
    "    print(\"\\n\" + word)\n",
    "    print(f\"-{male_word}: \\t\", cos_similarity(word_id, male_word_id, model).numpy())\n",
    "    print(f\"-{female_word}: \\t\", cos_similarity(word_id, female_word_id, model).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:EVALUATOR:most similar to two: , side, aim, impurity, wonder, three, term, food, trust, pray, anybody\n"
     ]
    }
   ],
   "source": [
    "word_id = word2id[\"two\"]\n",
    "_ = get_similar_words(model, word_id, vocab_size, n, id2word=id2word, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:EVALUATOR:most similar to house: , attractive, eas, past, vanity, therewith, allowed, illegal, violence, flaming, announced\n"
     ]
    }
   ],
   "source": [
    "word_id = word2id[\"house\"]\n",
    "_ = get_similar_words(model, word_id, vocab_size, n, id2word=id2word, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:EVALUATOR:most similar to punish: , risen, displeases, scorned, unconcerned, correctly, cunning, repeat, contains, dispensed, fodder\n"
     ]
    }
   ],
   "source": [
    "word_id = word2id[\"punish\"]\n",
    "_ = get_similar_words(model, word_id, vocab_size, n, id2word=id2word, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:EVALUATOR:most similar to food: , teaching, dealt, replace, cunning, babble, chastise, bond, eaten, worked, closer\n"
     ]
    }
   ],
   "source": [
    "word_id = word2id[\"food\"]\n",
    "_ = get_similar_words(model, word_id, vocab_size, n, id2word=id2word, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
